{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assigment 1 - David.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"66048pM0QRvv","colab_type":"text"},"source":["# Indexing for Web Search\n","## Information Retrieval Assignment 1"]},{"cell_type":"markdown","metadata":{"id":"f8BxJhtYQRvz","colab_type":"text"},"source":["Installing and importing the required packages and libraries"]},{"cell_type":"code","metadata":{"id":"2f89gdPgQRv3","colab_type":"code","outputId":"bb1bafbd-d23c-4441-aac3-2d1eb420ba4d","executionInfo":{"status":"ok","timestamp":1581975885122,"user_tz":0,"elapsed":7109,"user":{"displayName":"Gurkirat Sarna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvINaawDRDQGEGuMzkPnXyJ9r4QEJuaSCEniiWOg=s64","userId":"12289269705463968995"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["pip install spacy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hi7ppYlNgdRh","colab_type":"code","outputId":"ef943624-c66b-4112-ba42-b15ae7c7be9a","executionInfo":{"status":"ok","timestamp":1581978494329,"user_tz":0,"elapsed":3655,"user":{"displayName":"Gurkirat Sarna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvINaawDRDQGEGuMzkPnXyJ9r4QEJuaSCEniiWOg=s64","userId":"12289269705463968995"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pip install inflect"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (2.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lHk847DgQRwK","colab_type":"code","colab":{}},"source":["import nltk, re\n","import sklearn\n","import urllib\n","import urllib.request\n","import urllib.parse\n","import numpy as np\n","import pandas as pd\n","import collections\n","import sys\n","from bs4 import BeautifulSoup\n","from nltk import sent_tokenize, word_tokenize\n","import inflect\n","import string, unicodedata\n","from nltk.corpus import stopwords\n","from nltk.stem import LancasterStemmer, WordNetLemmatizer\n","from nltk import pos_tag\n","from nltk import ne_chunk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClMDDocgQRwT","colab_type":"code","outputId":"28f6ccd7-9ff2-4b65-d2a2-335a389ad07c","executionInfo":{"status":"ok","timestamp":1581980781182,"user_tz":0,"elapsed":7063,"user":{"displayName":"Gurkirat Sarna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvINaawDRDQGEGuMzkPnXyJ9r4QEJuaSCEniiWOg=s64","userId":"12289269705463968995"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["'''----Installing the nltk package----\n","please follow the prompted instructions after execution of this line of code for successfull download of nltk packages.\n","'''\n","nltk.download()\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> q\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"GfEoqab_YRuU","colab_type":"code","outputId":"2de7556a-23a3-42f4-d2a6-93a0e3703d99","executionInfo":{"status":"ok","timestamp":1581978761007,"user_tz":0,"elapsed":17331,"user":{"displayName":"Gurkirat Sarna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvINaawDRDQGEGuMzkPnXyJ9r4QEJuaSCEniiWOg=s64","userId":"12289269705463968995"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["'''----get a list of html links/ URL's from the user that need to be processed---\n","'''\n","\n","html_link_list=[]\n","user_input='Yes'\n","\n","while(user_input=='Yes' or user_input=='yes'):\n","  html_link_list.append(input(\"Please enter the web link : \"))\n","  user_input=input(\"Do you wish to enter more html links ? (yes/NO) : \")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Please enter the web link : http://www.multimediaeval.org/mediaeval2019/memorability/\n","Do you wish to enter more html links ? (yes/NO) : yes\n","Please enter the web link : https://sites.google.com/view/siirh2020/\n","Do you wish to enter more html links ? (yes/NO) : \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xImEOWppR2Ij","colab_type":"code","outputId":"9ee1f64b-be3d-4b7a-8da1-403e24455e51","executionInfo":{"status":"error","timestamp":1582058110455,"user_tz":0,"elapsed":882,"user":{"displayName":"Gurkirat Sarna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvINaawDRDQGEGuMzkPnXyJ9r4QEJuaSCEniiWOg=s64","userId":"12289269705463968995"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["wordset=[]\n","for i in range(len(html_link_list)):\n","  stem_list=preprocessing(html_link_list[i])\n","  print(stem_list)\n","  wordset.append(stem_list)\n","  print(wordset)\n","print (\"WORDSET \\n\",wordset)\n","wordDictA = dict.fromkeys(wordSet, 0) \n","wordDictB = dict.fromkeys(wordSet, 0)\n","\n","for word in wordset[0]:\n","    wordDictA[word]+=1\n","    \n","for word in worset[1]:\n","    wordDictB[word]+=1\n","\n","print(pd.DataFrame([wordDictA, wordDictB]))\n","\n","#Compute the function for the two documents\n","tfBowA = computeTF(wordDictA, stems1)\n","tfBowB = computeTF(wordDictB, stems2)\n","\n","#Computing the IDF for both documents\n","idfs = computeIDF([wordDictA, wordDictB])\n","\n","#Calculating the TF.IDF score for both documents\n","tfidfBowA = computeTFIDF(tfBowA, idfs)\n","tfidfBowB = computeTFIDF(tfBowB, idfs)\n","\n","#Data frame of the scores\n","score = pd.DataFrame([tfidfBowA, tfidfBowB])\n","print(scores)\n","\n","#Sort the scores (to know what words are more relevant in each document)\n","\n","#Document 1 (Web page 1)\n","score1 = pd.DataFrame([tfidfBowA])\n","scorest1 = score1.T\n","scorest1.columns = ['tf.idf(doc1)']\n","\n","scorestf1 = scorest1.sort_values(['tf.idf(doc1)'], ascending = False)\n","print(scorestf1)\n","\n","#-------\n","\n","#Document 2 (Web page 2)\n","score2 = pd.DataFrame([tfidfBowB])\n","scorest2 = score2.T\n","scorest2.columns = ['tf.idf(doc2)']\n","\n","scorestf2 = scorest2.sort_values(['tf.idf(doc2)'], ascending = False)\n","print(scorestf2)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-055fd5da6400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwordset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_link_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mstem_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_link_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mwordset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'html_link_list' is not defined"]}]},{"cell_type":"code","metadata":{"id":"czQ5uK_ukrJC","colab_type":"code","colab":{}},"source":["'''----Following are performed in the below code :\n","'''\n","def preprocessing(html_link):\n","\n","  \n","  #----HTML Parsing----\n","\n","  #Reading web page by calling read_web_page() function\n","  html=read_web_page(html_link)\n","  \n","  #Getting the plain text from the web page read above by calling get_html_text() function\n","  text=get_html_text(html)\n","  \n","  #----Pre-processing: Sentence Splitting, Tokenization and Normalization----\n","\n","  #Split the sentence by calling sentence_splitting() function. It returns a list of strings separated by '. '.\n","  split=sentence_splitting(text)\n","\n","  '''Tokenize the words in the sentences by using tokenize_word function. This function returns a list of lists after splitting the words in every\n","  sentence into \n","  '''\n","  list_token=[]\n","  list_token=tokenize_word(split)\n","  \n","  #Normalization\n","  norm_tokens = []\n","  for j in list_token:\n","    norm_token = normalize(j)\n","    norm_tokens.append(norm_token)\n","  \n","  #Convert list of lists to one lists\n","  norm_tokens_list = convert_listoflists(norm_tokens)\n","\n","  #remove English Language stopwords - Fro example 'for', 'the', 'in' etc\n","  filtered_tokens = delete_stopwords(norm_tokens_list)\n","  \n","  #----Part-of-Speech Tagging----\n","  tag = nltk.pos_tag(filtered_tokens)\n","  print(\"TAGS \\n\",tag)\n","  \n","  #Entity recognition\n","  entity_reco_list = entity_reco(tag)\n","  print(\"ENTITY RECO \\n\",entity_reco_list)\n","\n","  #----lemmatization and Stemming----\n","\n","  #lemmatization\n","  lemmas = lemmatization(tag)\n","  print(\"LEMMAS ]\\n\",lemmas)\n","\n","  #Stemming\n","  stems = stemming(lemmas)\n","  print(\"STEMS \\n\",stems)\n","\n","  return stems"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_MZO8PhgF3P","colab_type":"code","colab":{}},"source":["def read_web_page(html_link):\n","  return urllib.request.urlopen(html_link).read().decode('utf8')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQo5bQO8iRyJ","colab_type":"code","colab":{}},"source":["def get_html_text(html_link):\n","  \n","  soup = BeautifulSoup(html_link)\n","  \n","  '''---Delete script and style---\n","  This helps in removing the <script> and <style> html tag data and read that text as normal words. For example, when getting the text from the html_link\n","  'Media Memorability', the 'Contact' hyperlink at the end is not expanded to include the javascript as below-\n","  \"var _rwObsfuscatedHref0 = \"mai\";var _rwObsfuscatedHref1 = \"lto\";var _rwObsfuscatedHref2 = \":m.\";var _rwObsfuscatedHref3 = \"a.l\";\n","  var _rwObsfuscatedHref4 = \"ars\";var _rwObsfuscatedHref5 = \"on@\";var _rwObsfuscatedHref6 = \"tud\";var _rwObsfuscatedHref7 = \"elf\";\n","  var _rwObsfuscatedHref8 = \"t.n\";var _rwObsfuscatedHref9 = \"l\";  var _rwObsfuscatedHref = _rwObsfuscatedHref0+_rwObsfuscatedHref1+_rwObsfuscatedHref2+\n","  _rwObsfuscatedHref3+_rwObsfuscatedHref4+_rwObsfuscatedHref5+_rwObsfuscatedHref6+_rwObsfuscatedHref7+_rwObsfuscatedHref8+_rwObsfuscatedHref9; \n","  document.getElementById('rw_email_contact').href = _rwObsfuscatedHref;\"\n","  Instead it is read as normal text word 'Contact'.\n","  '''\n","  for script in soup([\"script\", \"style\"]):\n","    script.extract()\n","    \n","  #----Get all the text from the html_link passed to this function.----\n","  text = soup.get_text()\n","  \n","  #---- Remove leading and trailing spaces in each line----\n","  lines = (line.strip() for line in text.splitlines())\n","    \n","  #----break multi-headlines into each line----\n","  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n","  \n","  #----remove blank lines and club the text----\n","  text = '\\n'.join(chunk for chunk in chunks if chunk)\n","  \n","  return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S88sq9xB2aHX","colab_type":"code","colab":{}},"source":["#----Sentence Splitting function. The tokenize function splits the text by '. ' and returns a list of these strings.----\n","\n","def sentence_splitting(text):\n","  split = sent_tokenize(text)\n","  return split\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLsajrM973hh","colab_type":"code","colab":{}},"source":["def tokenize_word(split_list):\n","  list_token=[]\n","  for i in split_list:\n","    token = word_tokenize(i)\n","    list_token.append(token)\n","  return list_token"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEX7RVZbQRw9","colab_type":"code","colab":{}},"source":["#Replacing numbers into characters. For example 9 becomes 'nine'.\n","def rep_numbers(token):\n","    p = inflect.engine()\n","    words = []\n","    for i in token:\n","        if i.isdigit():\n","            new_word = p.number_to_words(i)\n","            words.append(new_word)\n","        else:\n","            words.append(i)\n","    return words\n","\n","\n","#Change the all words to lowercase\n","def lowerc(token):\n","    words = []\n","    for i in token:\n","        nrm_word = i.lower()\n","        words.append(nrm_word)\n","    return words\n","\n","#Remove punctuation\n","def punct(token):\n","    words = []\n","    for i in token:\n","        nrm_word = re.sub(r'[^\\w\\s]','', i)\n","        if nrm_word != '':\n","            words.append(nrm_word)\n","    return words\n","\n","#Call the above three functions from one function, this function is called from the main function.\n","def normalize(token):\n","    token = rep_numbers(token)\n","    token = lowerc(token)\n","    token = punct(token)\n","    return token"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F0xQ0AoT_ifl","colab_type":"code","colab":{}},"source":["def convert_listoflists(token_listoflists):\n","  norm_list = []\n","  for i in token_listoflists:\n","    for j in i:\n","        norm_list.append(j)\n","  return norm_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOk0n3PrF5E7","colab_type":"code","colab":{}},"source":["def delete_stopwords(token_list):\n","  \n","  #List of English language Stopwords\n","  stop_words = set(stopwords.words('english'))\n","\n","  #Filtering stopwords from the token list\n","  filtered = []\n","  for i in token_list:\n","    if i not in stop_words:\n","        filtered.append(i)\n","  return filtered"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjRMmRbrQRxX","colab_type":"code","colab":{}},"source":["def lemmatization(filtered):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = []\n","    for i in filtered:\n","        lemma = lemmatizer.lemmatize(i, pos='v')\n","        lemmas.append(lemma)\n","    return lemmas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPGOxdVKQRxe","colab_type":"code","colab":{}},"source":["def stemming(lemmas):\n","    stemmer = LancasterStemmer()\n","    stems = []\n","    for i in lemmas:\n","        stem = stemmer.stem(i)\n","        stems.append(stem)\n","    return stems"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TIpdPmyOdrT","colab_type":"code","colab":{}},"source":["#Entity recognition\n","def entity_reco(tag):\n","  chu = ne_chunk(tag, binary=True)\n","  return chu"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zA3sBAMdQRxx","colab_type":"text"},"source":["## Selecting Keyword"]},{"cell_type":"code","metadata":{"id":"h1bpKnILQRxy","colab_type":"code","outputId":"d4a5aae4-43a1-45cc-a060-c576563d0c75","colab":{}},"source":["#Counting the frequency of each word for each document\n","wordSet = set(stems1).union(set(stems2))\n","\n","wordDictA = dict.fromkeys(wordSet, 0) \n","wordDictB = dict.fromkeys(wordSet, 0)\n","\n","for word in stems1:\n","    wordDictA[word]+=1\n","    \n","for word in stems2:\n","    wordDictB[word]+=1\n","\n","pd.DataFrame([wordDictA, wordDictB])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pap</th>\n","      <th>easychair</th>\n","      <th>francisco</th>\n","      <th>comput</th>\n","      <th>brain</th>\n","      <th>research</th>\n","      <th>task</th>\n","      <th>context</th>\n","      <th>inform</th>\n","      <th>resourc</th>\n","      <th>...</th>\n","      <th>frant</th>\n","      <th>multimed</th>\n","      <th>platform</th>\n","      <th>liu</th>\n","      <th>lamuria</th>\n","      <th>hospit</th>\n","      <th>2019working</th>\n","      <th>soundless</th>\n","      <th>rinald</th>\n","      <th>1159pm</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 736 columns</p>\n","</div>"],"text/plain":["   pap  easychair  francisco  comput  brain  research  task  context  inform  \\\n","0    1          0          0       4      1         0     6        1       1   \n","1    8          1          1       1      0         3     3        0       5   \n","\n","   resourc  ...  frant  multimed  platform  liu  lamuria  hospit  2019working  \\\n","0        0  ...      4        11         2    1        0       0            1   \n","1        2  ...      0         0         0    0        1       1            0   \n","\n","   soundless  rinald  1159pm  \n","0          1       0       0  \n","1          0       1       3  \n","\n","[2 rows x 736 columns]"]},"metadata":{"tags":[]},"execution_count":152}]},{"cell_type":"code","metadata":{"id":"_snh6ACRQRx2","colab_type":"code","colab":{}},"source":["#Creating the function to calculate the TF\n","def computeTF (wordDict, bow):\n","    tfDict = {}\n","    bowCount = len(bow)\n","    for word, count in wordDict.items():\n","        tfDict[word]=count/float(bowCount)\n","    return tfDict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0c0d-vIQRx7","colab_type":"code","colab":{}},"source":["#Compute the function for the two documents\n","tfBowA = computeTF(wordDictA, stems1)\n","tfBowB = computeTF(wordDictB, stems2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzg_IvaWQRyD","colab_type":"code","colab":{}},"source":["#Create the fucntion to calculate the IDF\n","def computeIDF(docList):\n","    import math\n","    idfDict = {}\n","    N = len(docList)\n","    \n","    idfDict = dict.fromkeys(docList[0].keys(), 0)\n","    for doc in docList:\n","        for word, val in doc.items():\n","            if val > 0:\n","                idfDict[word] += 1\n","    \n","    for word, val in idfDict.items():\n","        idfDict[word] = math.log10(N / float(val))\n","        \n","    return idfDict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1z4pbmNcQRyK","colab_type":"code","colab":{}},"source":["#Computing the IDF for both documents\n","idfs = computeIDF([wordDictA, wordDictB])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__gNbIjgQRyR","colab_type":"code","colab":{}},"source":["#Create the function to calculate the TF.IDF score\n","def computeTFIDF(tfBow, idfs):\n","    tfidf = {}\n","    for word, val in tfBow.items():\n","        tfidf[word] = val*idfs[word]\n","    return tfidf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5rG7flHQRyY","colab_type":"code","colab":{}},"source":["#Calculating the TF.IDF score for both documents\n","tfidfBowA = computeTFIDF(tfBowA, idfs)\n","tfidfBowB = computeTFIDF(tfBowB, idfs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aX4qI1a4QRye","colab_type":"code","outputId":"a651e1ef-23b8-4d5c-a858-1b8757b4c3cc","colab":{}},"source":["#Data frame of the scores\n","score = pd.DataFrame([tfidfBowA, tfidfBowB])\n","scores"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pap</th>\n","      <th>easychair</th>\n","      <th>francisco</th>\n","      <th>comput</th>\n","      <th>brain</th>\n","      <th>research</th>\n","      <th>task</th>\n","      <th>context</th>\n","      <th>inform</th>\n","      <th>resourc</th>\n","      <th>...</th>\n","      <th>frant</th>\n","      <th>multimed</th>\n","      <th>platform</th>\n","      <th>liu</th>\n","      <th>lamuria</th>\n","      <th>hospit</th>\n","      <th>2019working</th>\n","      <th>soundless</th>\n","      <th>rinald</th>\n","      <th>1159pm</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.00043</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.00043</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.00172</td>\n","      <td>0.00473</td>\n","      <td>0.00086</td>\n","      <td>0.00043</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00043</td>\n","      <td>0.00043</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.000481</td>\n","      <td>0.000481</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.001443</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.000962</td>\n","      <td>...</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.000481</td>\n","      <td>0.000481</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.000481</td>\n","      <td>0.001443</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 736 columns</p>\n","</div>"],"text/plain":["   pap  easychair  francisco  comput    brain  research  task  context  \\\n","0  0.0   0.000000   0.000000     0.0  0.00043  0.000000   0.0  0.00043   \n","1  0.0   0.000481   0.000481     0.0  0.00000  0.001443   0.0  0.00000   \n","\n","   inform   resourc  ...    frant  multimed  platform      liu   lamuria  \\\n","0     0.0  0.000000  ...  0.00172   0.00473   0.00086  0.00043  0.000000   \n","1     0.0  0.000962  ...  0.00000   0.00000   0.00000  0.00000  0.000481   \n","\n","     hospit  2019working  soundless    rinald    1159pm  \n","0  0.000000      0.00043    0.00043  0.000000  0.000000  \n","1  0.000481      0.00000    0.00000  0.000481  0.001443  \n","\n","[2 rows x 736 columns]"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"code","metadata":{"id":"xN6S52n4QRym","colab_type":"code","colab":{}},"source":["#Sort the scores (to know what words are more relevant in each document)\n","\n","#Document 1 (Web page 1)\n","score1 = pd.DataFrame([tfidfBowA])\n","scorest1 = score1.T\n","scorest1.columns = ['tf.idf(doc1)']\n","\n","scorestf1 = scorest1.sort_values(['tf.idf(doc1)'], ascending = False)\n","#print(scorestf1)\n","\n","#-------\n","\n","#Document 2 (Web page 2)\n","score2 = pd.DataFrame([tfidfBowB])\n","scorest2 = score2.T\n","scorest2.columns = ['tf.idf(doc2)']\n","\n","scorestf2 = scorest2.sort_values(['tf.idf(doc2)'], ascending = False)\n","#print(scorestf2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsnefpcSQRyt","colab_type":"code","colab":{}},"source":["#Exportin the results to csv\n","\n","scorestf1.to_csv('/Users/davidalbuja/Documents/Essex/Spring/Information Retrieval/Assigment 1/Outputs/TFIDF(doc1).csv')\n","scorestf2.to_csv('/Users/davidalbuja/Documents/Essex/Spring/Information Retrieval/Assigment 1/Outputs/TFIDF(doc2).csv') "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9PXBYguQRy1","colab_type":"text"},"source":["### Entity recognition"]},{"cell_type":"code","metadata":{"id":"dbPW1ADjQRy3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fW9QbiXYQRy9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}