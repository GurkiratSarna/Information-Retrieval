{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assigment 1 - David.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurkiratSarna/Information-Retrieval/blob/master/Assigment_1_1900690.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66048pM0QRvv",
        "colab_type": "text"
      },
      "source": [
        "# Indexing for Web Search\n",
        "## Information Retrieval Assignment 1\n",
        "### Registeration ID's :\n",
        "### 1900690\n",
        "### 1900863"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BxJhtYQRvz",
        "colab_type": "text"
      },
      "source": [
        "Installing and importing the required packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f89gdPgQRv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Umcomment and execute only once\n",
        "#pip install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi7ppYlNgdRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Umcomment and execute only once\n",
        "#pip install inflect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHk847DgQRwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk, re\n",
        "import sklearn\n",
        "import urllib\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import sys\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "import inflect\n",
        "import string, unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "#from google.colab import files\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClMDDocgQRwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''----Installing the nltk package----\n",
        "please follow the prompted instructions after execution of this line of code for successfull download of nltk packages.\n",
        "'''\n",
        "nltk.download()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMf6I0KOrX4O",
        "colab_type": "text"
      },
      "source": [
        "Below is the main cell which has to be executed to get the outputs.\n",
        "Note : All the other cells need to be run before executing the below cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYJkkp71tXYQ",
        "colab_type": "code",
        "outputId": "b0c2ab6b-aeaf-4cd4-caec-0b0cab3cc57d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Get web pages fromt he user\n",
        "html_link=[]\n",
        "html_links=get_webpages()\n",
        "\n",
        "#Preprocessing to get the stems of all the web pages\n",
        "stems=[]\n",
        "stems=preprocessing(html_links)\n",
        "\n",
        "#POS Tagging\n",
        "tag=[]\n",
        "tag=pos_tagging(stems)\n",
        "\n",
        "#Entity Recognition\n",
        "NER=[]\n",
        "NER=entity_reco(tag)\n",
        "\n",
        "#Create a dictionary for all the webpages\n",
        "var_dict=var_dictionary(html_links)\n",
        "\n",
        "#Get the word count in each web page\n",
        "var_dict=word_freq(var_dict)\n",
        "\n",
        "#Calculate Term Frequency\n",
        "stem_list_index=0\n",
        "TFBow={**var_dict}\n",
        "#Call computeTF function to for each dictionary value.\n",
        "for key, TFBowval in TFBow.items():\n",
        "  TFBow[key]=computeTF(TFBowval,stem_list[stem_list_index])\n",
        "  stem_list_index +=1\n",
        "\n",
        "#For displaying properly as a data frame\n",
        "test=pd.DataFrame(TFBow)\n",
        "test=test.transpose()\n",
        "display(test)\n",
        "\n",
        "#Computing the IDF for both documents\n",
        "\n",
        "#Create a list of dictionary variables\n",
        "temp_list=[]\n",
        "for k, v in var_dict.items():\n",
        "  temp_list.append(d[k])\n",
        "idfs = computeIDF(temp_list)\n",
        "\n",
        "#Calculating the TF.IDF score for all documents\n",
        "tfidfBow={**TFBow}\n",
        "for k, v in tfidfBow.items():\n",
        "    tfidfBow[k]=computeTFIDF(v, idfs)\n",
        "\n",
        "\n",
        "#Data frame of the scores\n",
        "#For displaying properly as a data frame\n",
        "score=pd.DataFrame(tfidfBow)\n",
        "score=score.transpose()\n",
        "display(score)\n",
        "\n",
        "for key, val in tfidfBow.items():\n",
        "  scorest=sort_score(val)\n",
        "  #To download the final score files uncomment the below line, for every document a pop up will ask for the location where the file has to be stored.\n",
        "  #download_file(scorest)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the web link : http://www.multimediaeval.org/mediaeval2019/memorability/\n",
            "Do you wish to enter more html links ? (yes/NO) : yes\n",
            "Please enter the web link : https://sites.google.com/view/siirh2020/\n",
            "Do you wish to enter more html links ? (yes/NO) : \n",
            "\n",
            " In the function HTML LIST : \n",
            "['http://www.multimediaeval.org/mediaeval2019/memorability/', 'https://sites.google.com/view/siirh2020/']\n",
            "In MAIN, html link : \n",
            "['http://www.multimediaeval.org/mediaeval2019/memorability/', 'https://sites.google.com/view/siirh2020/']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ad1cb183f51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In MAIN, stems : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e01cc0e3045a>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(html_link)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#Reading web page by calling read_web_page() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mhtml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_web_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m#Getting the plain text from the web page read above by calling get_html_text() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5f7478208630>\u001b[0m in \u001b[0;36mread_web_page\u001b[0;34m(html_link)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_web_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'timeout'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfEoqab_YRuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''----get a list of html links/ URL's from the user that need to be processed---\n",
        "'''\n",
        "def get_webpages():\n",
        "  html_link_list=[]\n",
        "  user_input='Yes'\n",
        "\n",
        "  while(user_input=='Yes' or user_input=='yes'):\n",
        "    html_link_list.append(input(\"Please enter the web link : \"))\n",
        "    user_input=input(\"Do you wish to enter more html links ? (yes/NO) : \")\n",
        "  print(\"\\n In the function HTML LIST : \")\n",
        "  print(html_link_list)\n",
        "  return html_link_list\n",
        "\n",
        "#Following can be uncommented to download the HTML lists. You will have a prompt to select the destination and the file name.\n",
        "#download_file(html_link_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czQ5uK_ukrJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''----Following are performed in the below code :\n",
        "'''\n",
        "def preprocessing(html_link):\n",
        "  \n",
        "  #----HTML Parsing----\n",
        "\n",
        "  #Reading web page by calling read_web_page() function\n",
        "  html=read_web_page(html_link)\n",
        "  \n",
        "  #Getting the plain text from the web page read above by calling get_html_text() function\n",
        "  text=get_html_text(html)\n",
        "  \n",
        "  #----Pre-processing: Sentence Splitting, Tokenization, Normalization and removal of stopwords----\n",
        "\n",
        "  #Split the sentence by calling sentence_splitting() function. It returns a list of strings separated by '. '.\n",
        "  split=sentence_splitting(text)\n",
        "\n",
        "  '''Tokenize the words in the sentences by using tokenize_word function. This function returns a list of lists after splitting the words in every\n",
        "  sentence.\n",
        "  '''\n",
        "  list_token=[]\n",
        "  list_token=tokenize_word(split)\n",
        "  \n",
        "  #Normalization\n",
        "  norm_tokens = []\n",
        "  for j in list_token:\n",
        "    norm_token = normalize(j)\n",
        "    norm_tokens.append(norm_token)\n",
        "  \n",
        "  #Convert list of lists to one lists\n",
        "  norm_tokens_list = convert_listoflists(norm_tokens)\n",
        " \n",
        "  #remove English Language stopwords - Fro example 'for', 'the', 'in' etc\n",
        "  filtered_tokens = delete_stopwords(norm_tokens_list)\n",
        " \n",
        "  #----lemmatization and Stemming----\n",
        "  lemmas = lemmatization(filtered_tokens)\n",
        " \n",
        "  stems=[]\n",
        "  #Stemming\n",
        "  stems = stemming(lemmas)\n",
        "  return stems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCnzv4kRwUtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_tagging(stem_list):\n",
        "    #----Part-of-Speech Tagging----\n",
        "    tag = nltk.pos_tag(stem_list)\n",
        "    return tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou6RWCMP3wdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----Entity recognition----\n",
        "def entity_reco(tag):\n",
        "  entity_reco_list = entity_reco1(tag)\n",
        "  return entity_reco_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_MZO8PhgF3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The function read the HTML pages and decodes them in utf-8 format. The format is usually present in the meta inormation of the web page.\n",
        "def read_web_page(html_link):\n",
        "  return urllib.request.urlopen(html_link).read().decode('utf8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQo5bQO8iRyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_html_text(html_link):\n",
        "  \n",
        "  soup = BeautifulSoup(html_link)\n",
        "  \n",
        "  '''---Delete script and style---\n",
        "  This helps in removing the <script> and <style> html tag data and read that text as normal words. For example, when getting the text from the html_link\n",
        "  'Media Memorability', the 'Contact' hyperlink at the end is not expanded to include the javascript as below-\n",
        "  \"var _rwObsfuscatedHref0 = \"mai\";var _rwObsfuscatedHref1 = \"lto\";var _rwObsfuscatedHref2 = \":m.\";var _rwObsfuscatedHref3 = \"a.l\";\n",
        "  var _rwObsfuscatedHref4 = \"ars\";var _rwObsfuscatedHref5 = \"on@\";var _rwObsfuscatedHref6 = \"tud\";var _rwObsfuscatedHref7 = \"elf\";\n",
        "  var _rwObsfuscatedHref8 = \"t.n\";var _rwObsfuscatedHref9 = \"l\";  var _rwObsfuscatedHref = _rwObsfuscatedHref0+_rwObsfuscatedHref1+_rwObsfuscatedHref2+\n",
        "  _rwObsfuscatedHref3+_rwObsfuscatedHref4+_rwObsfuscatedHref5+_rwObsfuscatedHref6+_rwObsfuscatedHref7+_rwObsfuscatedHref8+_rwObsfuscatedHref9; \n",
        "  document.getElementById('rw_email_contact').href = _rwObsfuscatedHref;\"\n",
        "  Instead it is read as normal text word 'Contact'.\n",
        "  '''\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "    \n",
        "  #----Get all the text from the html_link passed to this function.----\n",
        "  text = soup.get_text()\n",
        "  \n",
        "  #---- Remove leading and trailing spaces in each line----\n",
        "  lines = (line.strip() for line in text.splitlines())\n",
        "    \n",
        "  #----break multi-headlines into each line----\n",
        "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "  \n",
        "  #----remove blank lines and club the text----\n",
        "  text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "  \n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S88sq9xB2aHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----Sentence Splitting function. The tokenize function splits the text by '. ' and returns a list of these strings.----\n",
        "\n",
        "def sentence_splitting(text):\n",
        "  split = sent_tokenize(text)\n",
        "  return split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLsajrM973hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_word(split_list):\n",
        "  list_token=[]\n",
        "  for i in split_list:\n",
        "    token = word_tokenize(i)\n",
        "    list_token.append(token)\n",
        "  return list_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEX7RVZbQRw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replacing numbers into characters. For example 9 becomes 'nine'.\n",
        "def rep_numbers(token):\n",
        "    p = inflect.engine()\n",
        "    words = []\n",
        "    for i in token:\n",
        "        if i.isdigit():\n",
        "            new_word = p.number_to_words(i)\n",
        "            words.append(new_word)\n",
        "        else:\n",
        "            words.append(i)\n",
        "    return words\n",
        "\n",
        "\n",
        "#Change the all words to lowercase\n",
        "def lowerc(token):\n",
        "    words = []\n",
        "    for i in token:\n",
        "        nrm_word = i.lower()\n",
        "        words.append(nrm_word)\n",
        "    return words\n",
        "\n",
        "#Remove punctuation\n",
        "def punct(token):\n",
        "    words = []\n",
        "    for i in token:\n",
        "        nrm_word = re.sub(r'[^\\w\\s]','', i)\n",
        "        if nrm_word != '':\n",
        "            words.append(nrm_word)\n",
        "    return words\n",
        "\n",
        "#Call the above three functions from one function, this function is called from the main function.\n",
        "def normalize(token):\n",
        "    token = rep_numbers(token)\n",
        "    token = lowerc(token)\n",
        "    token = punct(token)\n",
        "    return token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0xQ0AoT_ifl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_listoflists(token_listoflists):\n",
        "  norm_list = []\n",
        "  for i in token_listoflists:\n",
        "    for j in i:\n",
        "        norm_list.append(j)\n",
        "  return norm_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOk0n3PrF5E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def delete_stopwords(token_list):\n",
        "  \n",
        "  #List of English language Stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  #Filtering stopwords from the token list\n",
        "  filtered = []\n",
        "  for i in token_list:\n",
        "    if i not in stop_words:\n",
        "        filtered.append(i)\n",
        "  return filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjRMmRbrQRxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatization(filtered):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    #print(\"in lemmatization\")\n",
        "    for i in filtered:\n",
        "        #print(\"In for loop\")\n",
        "        lemma = lemmatizer.lemmatize(i, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    #print(\"\\n in lemmatization, type of lemmas returned : \", type(lemmas))\n",
        "    return lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPGOxdVKQRxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(lemmas):\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for i in lemmas:\n",
        "        stem = stemmer.stem(i)\n",
        "        stems.append(stem)\n",
        "    return stems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsdw4jSiQGFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_count(worddict, stemlist):\n",
        "  for word in stemlist:\n",
        "    worddict[word]+=1\n",
        "  return worddict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_snh6ACRQRx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the function to calculate the TF\n",
        "def computeTF (wordDict, bow):\n",
        "    tfDict = {}\n",
        "    bowCount = len(bow)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word]=count/float(bowCount)\n",
        "    return tfDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzg_IvaWQRyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the fucntion to calculate the IDF\n",
        "def computeIDF(docList):\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "    \n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "        for word, val in doc.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log10(N / float(val))\n",
        "        \n",
        "    return idfDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__gNbIjgQRyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the function to calculate the TF.IDF score\n",
        "def computeTFIDF(tfBow, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBow.items():\n",
        "        tfidf[word] = val*idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5orimkc1rY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_score(tfidfBowval):\n",
        "  score = pd.DataFrame([tfidfBowval])\n",
        "  scorest = score.T\n",
        "  scorest.columns = ['tf.idf(doc)']\n",
        "  scorestf = scorest.sort_values(['tf.idf(doc)'], ascending = False)\n",
        "  print(scorestf)\n",
        "  return scorestf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tE757nyKxmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file(filename):\n",
        "  filename=pd.DataFrame(filename)\n",
        "  filename.to_csv('Select Name while downloading.csv')\n",
        "  files.download('Select Name while downloading.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhijwWBaGUY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the list of all stems that were returned after stemming. Each list belong to each webpage.\n",
        "\n",
        "def var_dictionary(html_link_list):\n",
        "  stem_list=[]\n",
        "  for i in range(len(html_link_list)):\n",
        "    stem_list.append(preprocessing(html_link_list[i]))\n",
        "\n",
        "  #create a set of words from the list of stems that is combine all the stem lists and sort it\n",
        "  set_of_words={}\n",
        "  for i in range(len(stem_list)):\n",
        "    set_of_words=set(set_of_words).union(set(stem_list[i]))\n",
        "  set_of_words=sorted(set_of_words)\n",
        "\n",
        "  # Create a dictionary of dictionary type variables. The number of variables depends on the number of html links provided initially or the length of the stem list any which way.\n",
        "  var_dict={}\n",
        "  for x in range(len(stem_list)):\n",
        "    var_dict[\"wordDict{0}\".format(x)]={}\n",
        "\n",
        "  #Each dictionary variable created above is assigned the set of words with 0 value so it is easy to calculate the occurrences to calculate tf-idf\n",
        "  for key in var_dict.keys():\n",
        "    var_dict[key]=dict.fromkeys(set_of_words,0)\n",
        "  return var_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dveX2zmUj8mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For every dictionary belonging to every html link, call the word count function which provides the count for each word in its corresponding stem\n",
        "def word_freq(d):\n",
        "  stem_list_index=0\n",
        "  for key, dval in d.items():\n",
        "    d[key]=word_count(dval,stem_list[stem_list_index])\n",
        "    stem_list_index +=1\n",
        "\n",
        "  #For displaying properly as a data frame\n",
        "  test=pd.DataFrame(d)\n",
        "  test=test.transpose()\n",
        "  display(test)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}