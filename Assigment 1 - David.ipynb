{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (41.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.17.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (0.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.36.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (4.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from inflect) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->inflect) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/davidalbuja/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->inflect) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "import sklearn\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import sys\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Installing the package\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading Web pages\n",
    "#Web page 1\n",
    "url1 = \"http://www.multimediaeval.org/mediaeval2019/memorability/\"\n",
    "html1= urllib.request.urlopen(url1).read().decode('utf8')\n",
    "#Web page 2\n",
    "url2 = \"https://sites.google.com/view/siirh2020/\"\n",
    "html2= urllib.request.urlopen(url2).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the plain text\n",
    "#Parsing the web pages\n",
    "\n",
    "#Web page 1\n",
    "soup1 = BeautifulSoup(html1)\n",
    "\n",
    "# Delete script and style\n",
    "for script in soup1([\"script\", \"style\"]):\n",
    "    script.extract()\n",
    "\n",
    "# get text\n",
    "text1 = soup1.get_text()\n",
    "\n",
    "# Remove leading and trailing space in each line\n",
    "lines1 = (line.strip() for line in text1.splitlines())\n",
    "\n",
    "# break multi-headlines into each line\n",
    "chunks1 = (phrase.strip() for line in lines1 for phrase in line.split(\"  \"))\n",
    "\n",
    "# remove blank lines\n",
    "text1 = '\\n'.join(chunk for chunk in chunks1 if chunk)\n",
    "\n",
    "#print(text1)\n",
    "\n",
    "#---------\n",
    "\n",
    "#Web page 2\n",
    "soup2 = BeautifulSoup(html2)\n",
    "\n",
    "# Delete script and style\n",
    "for script in soup2([\"script\", \"style\"]):\n",
    "    script.extract()\n",
    "\n",
    "# get text\n",
    "text2 = soup2.get_text()\n",
    "\n",
    "# Remove leading and trailing space in each line\n",
    "lines2 = (line.strip() for line in text2.splitlines())\n",
    "\n",
    "# break multi-headlines into each line\n",
    "chunks2 = (phrase.strip() for line in lines2 for phrase in line.split(\"  \"))\n",
    "\n",
    "# remove blank lines\n",
    "text2 = '\\n'.join(chunk for chunk in chunks2 if chunk)\n",
    "\n",
    "#print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Sentence Splitting, Tokenization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Splitting\n",
    "from nltk import sent_tokenize\n",
    "#Web page 1\n",
    "split1 = sent_tokenize(text1)\n",
    "#Web page 2\n",
    "split2 = sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#Web page 1\n",
    "list_token1=[]\n",
    "for i in split1:\n",
    "    token1 = word_tokenize(i)\n",
    "    list_token1.append(token1)\n",
    "    \n",
    "#print(list_token1)\n",
    "\n",
    "#Web page 2\n",
    "list_token2=[]\n",
    "for i in split2:\n",
    "    token2 = word_tokenize(i)\n",
    "    list_token2.append(token2)\n",
    "    \n",
    "#print(list_token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "#Removing non-ASCII characters\n",
    "\n",
    "import inflect\n",
    "\n",
    "def rep_numbers(token):\n",
    "    p = inflect.engine()\n",
    "    words = []\n",
    "    for i in token:\n",
    "        if i.isdigit():\n",
    "            new_word = p.number_to_words(i)\n",
    "            words.append(new_word)\n",
    "        else:\n",
    "            words.append(i)\n",
    "    return words\n",
    "\n",
    "\n",
    "#Change words to lowercase\n",
    "def lowerc(token):\n",
    "    words = []\n",
    "    for i in token:\n",
    "        nrm_word = i.lower()\n",
    "        words.append(nrm_word)\n",
    "    return words\n",
    "\n",
    "#Remove punctuation\n",
    "def punct(token):\n",
    "    words = []\n",
    "    for i in token:\n",
    "        nrm_word = re.sub(r'[^\\w\\s]','', i)\n",
    "        if nrm_word != '':\n",
    "            words.append(nrm_word)\n",
    "    return words\n",
    "\n",
    "#Put everything together\n",
    "def normalize(token):\n",
    "    token = rep_numbers(token)\n",
    "    token = lowerc(token)\n",
    "    token = punct(token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the normalization function\n",
    "#Web page 1\n",
    "norm_tokens1 = []\n",
    "for i in list_token1:\n",
    "    norm_token1 = normalize(i)\n",
    "    norm_tokens1.append(norm_token1)\n",
    "    \n",
    "#Web page2\n",
    "norm_tokens2 = []\n",
    "for j in list_token2:\n",
    "    norm_token2 = normalize(j)\n",
    "    norm_tokens2.append(norm_token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting list of list into one list\n",
    "#Web page 1\n",
    "norm_list1 = []\n",
    "for i in norm_tokens1:\n",
    "    for j in i:\n",
    "        norm_list1.append(j)\n",
    "#print(norm_list1)\n",
    "\n",
    "#Web page 2\n",
    "norm_list2 = []\n",
    "for k in norm_tokens2:\n",
    "    for w in k:\n",
    "        norm_list2.append(w)\n",
    "#print(norm_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting stopwords\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "#English Stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Removin stopwords\n",
    "\n",
    "#Web page 1\n",
    "filtered1 = []\n",
    "for i in norm_list1:\n",
    "    if i not in stop_words:\n",
    "        filtered1.append(i)\n",
    "\n",
    "#print(filtered1)\n",
    "\n",
    "#Web page 2\n",
    "filtered2 = []\n",
    "for j in norm_list2:\n",
    "    if j not in stop_words:\n",
    "        filtered2.append(j)\n",
    "\n",
    "#print(filtered2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatimization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatimization\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "def lemmatimization(filtered):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for i in filtered:\n",
    "        lemma = lemmatizer.lemmatize(i, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "#Web page 1\n",
    "lemmas1 = lemmatimization(filtered1)\n",
    "#print(lemmas1)\n",
    "\n",
    "#Web page 2\n",
    "lemmas2 = lemmatimization(filtered2)\n",
    "#print(lemmas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "\n",
    "def stemming(lemmas):\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for i in lemmas:\n",
    "        stem = stemmer.stem(i)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "#Web page 1\n",
    "stems1 = stemming(lemmas1)\n",
    "#print(stems1)\n",
    "\n",
    "#Web page 2\n",
    "stems2 = stemming(lemmas2)\n",
    "#print(stems2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part-of-Speech Tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "#Web page 1\n",
    "tag1 = pos_tag(stems1)\n",
    "#print(tag1)\n",
    "\n",
    "#Web page 2\n",
    "tag2 = pos_tag(stems2)\n",
    "#print(tag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pap</th>\n",
       "      <th>easychair</th>\n",
       "      <th>francisco</th>\n",
       "      <th>comput</th>\n",
       "      <th>brain</th>\n",
       "      <th>research</th>\n",
       "      <th>task</th>\n",
       "      <th>context</th>\n",
       "      <th>inform</th>\n",
       "      <th>resourc</th>\n",
       "      <th>...</th>\n",
       "      <th>frant</th>\n",
       "      <th>multimed</th>\n",
       "      <th>platform</th>\n",
       "      <th>liu</th>\n",
       "      <th>lamuria</th>\n",
       "      <th>hospit</th>\n",
       "      <th>2019working</th>\n",
       "      <th>soundless</th>\n",
       "      <th>rinald</th>\n",
       "      <th>1159pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pap  easychair  francisco  comput  brain  research  task  context  inform  \\\n",
       "0    1          0          0       4      1         0     6        1       1   \n",
       "1    8          1          1       1      0         3     3        0       5   \n",
       "\n",
       "   resourc  ...  frant  multimed  platform  liu  lamuria  hospit  2019working  \\\n",
       "0        0  ...      4        11         2    1        0       0            1   \n",
       "1        2  ...      0         0         0    0        1       1            0   \n",
       "\n",
       "   soundless  rinald  1159pm  \n",
       "0          1       0       0  \n",
       "1          0       1       3  \n",
       "\n",
       "[2 rows x 736 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the frequency of each word for each document\n",
    "wordSet = set(stems1).union(set(stems2))\n",
    "\n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "for word in stems1:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in stems2:\n",
    "    wordDictB[word]+=1\n",
    "\n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the function to calculate the TF\n",
    "def computeTF (wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word]=count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the function for the two documents\n",
    "tfBowA = computeTF(wordDictA, stems1)\n",
    "tfBowB = computeTF(wordDictB, stems2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the fucntion to calculate the IDF\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the IDF for both documents\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the function to calculate the TF.IDF score\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the TF.IDF score for both documents\n",
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pap</th>\n",
       "      <th>easychair</th>\n",
       "      <th>francisco</th>\n",
       "      <th>comput</th>\n",
       "      <th>brain</th>\n",
       "      <th>research</th>\n",
       "      <th>task</th>\n",
       "      <th>context</th>\n",
       "      <th>inform</th>\n",
       "      <th>resourc</th>\n",
       "      <th>...</th>\n",
       "      <th>frant</th>\n",
       "      <th>multimed</th>\n",
       "      <th>platform</th>\n",
       "      <th>liu</th>\n",
       "      <th>lamuria</th>\n",
       "      <th>hospit</th>\n",
       "      <th>2019working</th>\n",
       "      <th>soundless</th>\n",
       "      <th>rinald</th>\n",
       "      <th>1159pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00172</td>\n",
       "      <td>0.00473</td>\n",
       "      <td>0.00086</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pap  easychair  francisco  comput    brain  research  task  context  \\\n",
       "0  0.0   0.000000   0.000000     0.0  0.00043  0.000000   0.0  0.00043   \n",
       "1  0.0   0.000481   0.000481     0.0  0.00000  0.001443   0.0  0.00000   \n",
       "\n",
       "   inform   resourc  ...    frant  multimed  platform      liu   lamuria  \\\n",
       "0     0.0  0.000000  ...  0.00172   0.00473   0.00086  0.00043  0.000000   \n",
       "1     0.0  0.000962  ...  0.00000   0.00000   0.00000  0.00000  0.000481   \n",
       "\n",
       "     hospit  2019working  soundless    rinald    1159pm  \n",
       "0  0.000000      0.00043    0.00043  0.000000  0.000000  \n",
       "1  0.000481      0.00000    0.00000  0.000481  0.001443  \n",
       "\n",
       "[2 rows x 736 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data frame of the scores\n",
    "score = pd.DataFrame([tfidfBowA, tfidfBowB])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the scores (to know what words are more relevant in each document)\n",
    "\n",
    "#Document 1 (Web page 1)\n",
    "score1 = pd.DataFrame([tfidfBowA])\n",
    "scorest1 = score1.T\n",
    "scorest1.columns = ['tf.idf(doc1)']\n",
    "\n",
    "scorestf1 = scorest1.sort_values(['tf.idf(doc1)'], ascending = False)\n",
    "#print(scorestf1)\n",
    "\n",
    "#-------\n",
    "\n",
    "#Document 2 (Web page 2)\n",
    "score2 = pd.DataFrame([tfidfBowB])\n",
    "scorest2 = score2.T\n",
    "scorest2.columns = ['tf.idf(doc2)']\n",
    "\n",
    "scorestf2 = scorest2.sort_values(['tf.idf(doc2)'], ascending = False)\n",
    "#print(scorestf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportin the results to csv\n",
    "\n",
    "scorestf1.to_csv('/Users/davidalbuja/Documents/Essex/Spring/Information Retrieval/Assigment 1/Outputs/TFIDF(doc1).csv')\n",
    "scorestf2.to_csv('/Users/davidalbuja/Documents/Essex/Spring/Information Retrieval/Assigment 1/Outputs/TFIDF(doc2).csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity recognition\n",
    "from nltk import ne_chunk\n",
    "\n",
    "chu1 = ne_chunk(tag1, binary=True)\n",
    "chu2 = ne_chunk(tag2, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
