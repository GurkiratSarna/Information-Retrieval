{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assigment 1 - David.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurkiratSarna/Information-Retrieval/blob/master/Assigment%201%20-%20David.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqU6SUP1ofe",
        "colab_type": "text"
      },
      "source": [
        "## Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uKicuUd1ofk",
        "colab_type": "text"
      },
      "source": [
        "### Importing the packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnJ6Xbb41ofn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i3F5lNY1ofz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install inflect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npxHG9731of6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk, re\n",
        "import sklearn\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import sys\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzOQSR4v1ogB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing the package\n",
        "nltk.download()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNcXQdn51ogI",
        "colab_type": "text"
      },
      "source": [
        "### HTML Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KfvNpPT1ogL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading Web pages\n",
        "#Web page 1\n",
        "url1 = \"http://www.multimediaeval.org/mediaeval2019/memorability/\"\n",
        "html1= urllib.request.urlopen(url1).read().decode('utf8')\n",
        "#Web page 2\n",
        "url2 = \"https://sites.google.com/view/siirh2020/\"\n",
        "html2= urllib.request.urlopen(url2).read().decode('utf8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNqFLI4n1ogS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the plain text\n",
        "#Parsing the web pages\n",
        "\n",
        "#Web page 1\n",
        "soup1 = BeautifulSoup(html1)\n",
        "\n",
        "# Delete script and style\n",
        "for script in soup1([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "\n",
        "# get text\n",
        "text1 = soup1.get_text()\n",
        "\n",
        "# Remove leading and trailing space in each line\n",
        "lines1 = (line.strip() for line in text1.splitlines())\n",
        "\n",
        "# break multi-headlines into each line\n",
        "chunks1 = (phrase.strip() for line in lines1 for phrase in line.split(\"  \"))\n",
        "\n",
        "# remove blank lines\n",
        "text1 = '\\n'.join(chunk for chunk in chunks1 if chunk)\n",
        "\n",
        "#print(text1)\n",
        "\n",
        "#---------\n",
        "\n",
        "#Web page 2\n",
        "soup2 = BeautifulSoup(html2)\n",
        "\n",
        "# Delete script and style\n",
        "for script in soup2([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "\n",
        "# get text\n",
        "text2 = soup2.get_text()\n",
        "\n",
        "# Remove leading and trailing space in each line\n",
        "lines2 = (line.strip() for line in text2.splitlines())\n",
        "\n",
        "# break multi-headlines into each line\n",
        "chunks2 = (phrase.strip() for line in lines2 for phrase in line.split(\"  \"))\n",
        "\n",
        "# remove blank lines\n",
        "text2 = '\\n'.join(chunk for chunk in chunks2 if chunk)\n",
        "\n",
        "#print(text2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5kgHiKe1ogX",
        "colab_type": "text"
      },
      "source": [
        "### Pre-processing: Sentence Splitting, Tokenization and Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEgcMAjH1ogY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting Splitting\n",
        "from nltk import sent_tokenize\n",
        "#Web page 1\n",
        "split1 = sent_tokenize(text1)\n",
        "#Web page 2\n",
        "split2 = sent_tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeEKvD7e1ogd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#Web page 1\n",
        "list_token1=[]\n",
        "for i in split1:\n",
        "    token1 = word_tokenize(i)\n",
        "    list_token1.append(token1)\n",
        "    \n",
        "#print(list_token1)\n",
        "\n",
        "#Web page 2\n",
        "list_token2=[]\n",
        "for i in split2:\n",
        "    token2 = word_tokenize(i)\n",
        "    list_token2.append(token2)\n",
        "    \n",
        "#print(list_token2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1vVRJUU1ogk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalization\n",
        "\n",
        "#Removing non-ASCII characters\n",
        "\n",
        "import inflect\n",
        "\n",
        "def rep_numbers(token):\n",
        "    p = inflect.engine()\n",
        "    words = []\n",
        "    for i in token:\n",
        "        if i.isdigit():\n",
        "            new_word = p.number_to_words(i)\n",
        "            words.append(new_word)\n",
        "        else:\n",
        "            words.append(i)\n",
        "    return words\n",
        "\n",
        "\n",
        "#Change words to lowercase\n",
        "def lowerc(token):\n",
        "    words = []\n",
        "    for i in token:\n",
        "        nrm_word = i.lower()\n",
        "        words.append(nrm_word)\n",
        "    return words\n",
        "\n",
        "#Remove punctuation\n",
        "def punct(token):\n",
        "    words = []\n",
        "    for i in token:\n",
        "        nrm_word = re.sub(r'[^\\w\\s]','', i)\n",
        "        if nrm_word != '':\n",
        "            words.append(nrm_word)\n",
        "    return words\n",
        "\n",
        "#Put everything together\n",
        "def normalize(token):\n",
        "    token = rep_numbers(token)\n",
        "    token = lowerc(token)\n",
        "    token = punct(token)\n",
        "    return token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wje2nc4K1ogs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calling the normalization function\n",
        "#Web page 1\n",
        "norm_tokens1 = []\n",
        "for i in list_token1:\n",
        "    norm_token1 = normalize(i)\n",
        "    norm_tokens1.append(norm_token1)\n",
        "    \n",
        "#Web page2\n",
        "norm_tokens2 = []\n",
        "for j in list_token2:\n",
        "    norm_token2 = normalize(j)\n",
        "    norm_tokens2.append(norm_token2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNlUZVTd1ogy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting list of list into one list\n",
        "#Web page 1\n",
        "norm_list1 = []\n",
        "for i in norm_tokens1:\n",
        "    for j in i:\n",
        "        norm_list1.append(j)\n",
        "#print(norm_list1)\n",
        "\n",
        "#Web page 2\n",
        "norm_list2 = []\n",
        "for k in norm_tokens2:\n",
        "    for w in k:\n",
        "        norm_list2.append(w)\n",
        "#print(norm_list2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8kFqez11og2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Deleting stopwords\n",
        "\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "#English Stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#Removin stopwords\n",
        "\n",
        "#Web page 1\n",
        "filtered1 = []\n",
        "for i in norm_list1:\n",
        "    if i not in stop_words:\n",
        "        filtered1.append(i)\n",
        "\n",
        "#print(filtered1)\n",
        "\n",
        "#Web page 2\n",
        "filtered2 = []\n",
        "for j in norm_list2:\n",
        "    if j not in stop_words:\n",
        "        filtered2.append(j)\n",
        "\n",
        "#print(filtered2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jaOosUe1og7",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatimization and Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuZXsDDm1og9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatimization\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "def lemmatimization(filtered):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for i in filtered:\n",
        "        lemma = lemmatizer.lemmatize(i, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "#Web page 1\n",
        "lemmas1 = lemmatimization(filtered1)\n",
        "#print(lemmas1)\n",
        "\n",
        "#Web page 2\n",
        "lemmas2 = lemmatimization(filtered2)\n",
        "#print(lemmas2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhsDlzkN1ohC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemming\n",
        "\n",
        "def stemming(lemmas):\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for i in lemmas:\n",
        "        stem = stemmer.stem(i)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "#Web page 1\n",
        "stems1 = stemming(lemmas1)\n",
        "#print(stems1)\n",
        "\n",
        "#Web page 2\n",
        "stems2 = stemming(lemmas2)\n",
        "#print(stems2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkXaMpgz1ohH",
        "colab_type": "text"
      },
      "source": [
        "### Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwoSnrBs1ohJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Part-of-Speech Tagging\n",
        "from nltk import pos_tag\n",
        "\n",
        "#Web page 1\n",
        "tag1 = pos_tag(stems1)\n",
        "#print(tag1)\n",
        "\n",
        "#Web page 2\n",
        "tag2 = pos_tag(stems2)\n",
        "#print(tag2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgFcfLxt1ohW",
        "colab_type": "text"
      },
      "source": [
        "## Selecting Keyword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ--_ncF1ohZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a75d216b-ccbe-4680-e5f1-6a4ca9b57413"
      },
      "source": [
        "#Counting the frequency of each word for each document\n",
        "wordSet = set(stems1).union(set(stems2))\n",
        "\n",
        "wordDictA = dict.fromkeys(wordSet, 0) \n",
        "wordDictB = dict.fromkeys(wordSet, 0)\n",
        "\n",
        "for word in stems1:\n",
        "    wordDictA[word]+=1\n",
        "    \n",
        "for word in stems2:\n",
        "    wordDictB[word]+=1\n",
        "\n",
        "pd.DataFrame([wordDictA, wordDictB])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>supercomput</th>\n",
              "      <th>sous</th>\n",
              "      <th>memorabilitysc</th>\n",
              "      <th>cohendet</th>\n",
              "      <th>lasig</th>\n",
              "      <th>read</th>\n",
              "      <th>2014mediaeval</th>\n",
              "      <th>investig</th>\n",
              "      <th>foot</th>\n",
              "      <th>thu</th>\n",
              "      <th>respons</th>\n",
              "      <th>thanhto</th>\n",
              "      <th>rec</th>\n",
              "      <th>italyalfonso</th>\n",
              "      <th>interest</th>\n",
              "      <th>swedenhyeju</th>\n",
              "      <th>greeceg</th>\n",
              "      <th>inform</th>\n",
              "      <th>compos</th>\n",
              "      <th>inf</th>\n",
              "      <th>patternstask</th>\n",
              "      <th>romaniabogd</th>\n",
              "      <th>stateoftheart</th>\n",
              "      <th>two thousand and seventeen</th>\n",
              "      <th>changesport</th>\n",
              "      <th>compil</th>\n",
              "      <th>stockholm</th>\n",
              "      <th>attend</th>\n",
              "      <th>mat</th>\n",
              "      <th>hammad</th>\n",
              "      <th>must</th>\n",
              "      <th>aud</th>\n",
              "      <th>two thousand and twenty</th>\n",
              "      <th>alberto</th>\n",
              "      <th>spainanastasio</th>\n",
              "      <th>francem</th>\n",
              "      <th>rapid</th>\n",
              "      <th>negoty</th>\n",
              "      <th>docu</th>\n",
              "      <th>comp</th>\n",
              "      <th>...</th>\n",
              "      <th>soph</th>\n",
              "      <th>romain</th>\n",
              "      <th>short</th>\n",
              "      <th>techn</th>\n",
              "      <th>ent</th>\n",
              "      <th>object</th>\n",
              "      <th>fut</th>\n",
              "      <th>fil</th>\n",
              "      <th>bioasq</th>\n",
              "      <th>larg</th>\n",
              "      <th>conceiv</th>\n",
              "      <th>changyu</th>\n",
              "      <th>portugalrafael</th>\n",
              "      <th>gwenaël</th>\n",
              "      <th>twentysev</th>\n",
              "      <th>pract</th>\n",
              "      <th>ked</th>\n",
              "      <th>auth</th>\n",
              "      <th>on</th>\n",
              "      <th>pubm</th>\n",
              "      <th>kim</th>\n",
              "      <th>trento</th>\n",
              "      <th>diff</th>\n",
              "      <th>constantin</th>\n",
              "      <th>grow</th>\n",
              "      <th>røst</th>\n",
              "      <th>parissaclay</th>\n",
              "      <th>in</th>\n",
              "      <th>eg</th>\n",
              "      <th>barro</th>\n",
              "      <th>org</th>\n",
              "      <th>issu</th>\n",
              "      <th>may</th>\n",
              "      <th>seventytwo</th>\n",
              "      <th>park</th>\n",
              "      <th>usayonghu</th>\n",
              "      <th>norweg</th>\n",
              "      <th>cho</th>\n",
              "      <th>backgroundth</th>\n",
              "      <th>thirtysix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 727 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   supercomput  sous  memorabilitysc  ...  cho  backgroundth  thirtysix\n",
              "0            0     0               1  ...    1             1          1\n",
              "1            2     1               0  ...    0             0          0\n",
              "\n",
              "[2 rows x 727 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_3S6BxT1ohe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the function to calculate the TF\n",
        "def computeTF (wordDict, bow):\n",
        "    tfDict = {}\n",
        "    bowCount = len(bow)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word]=count/float(bowCount)\n",
        "    return tfDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w7E2naK1ohl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute the function for the two documents\n",
        "tfBowA = computeTF(wordDictA, stems1)\n",
        "tfBowB = computeTF(wordDictB, stems2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIhi0GUp1ohq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the fucntion to calculate the IDF\n",
        "def computeIDF(docList):\n",
        "    import math\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "    \n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "        for word, val in doc.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log10(N / float(val))\n",
        "        \n",
        "    return idfDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Q0MMyW1ohv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Computing the IDF for both documents\n",
        "idfs = computeIDF([wordDictA, wordDictB])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjY8APfP1oh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the function to calculate the TF.IDF score\n",
        "def computeTFIDF(tfBow, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBow.items():\n",
        "        tfidf[word] = val*idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfYPVD6h1oh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating the TF.IDF score for both documents\n",
        "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
        "tfidfBowB = computeTFIDF(tfBowB, idfs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xII1A2O41oh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9a8b89c8-708b-450b-be73-04c60a438608"
      },
      "source": [
        "#Data frame of the scores\n",
        "score = pd.DataFrame([tfidfBowA, tfidfBowB])\n",
        "score"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>supercomput</th>\n",
              "      <th>sous</th>\n",
              "      <th>memorabilitysc</th>\n",
              "      <th>cohendet</th>\n",
              "      <th>lasig</th>\n",
              "      <th>read</th>\n",
              "      <th>2014mediaeval</th>\n",
              "      <th>investig</th>\n",
              "      <th>foot</th>\n",
              "      <th>thu</th>\n",
              "      <th>respons</th>\n",
              "      <th>thanhto</th>\n",
              "      <th>rec</th>\n",
              "      <th>italyalfonso</th>\n",
              "      <th>interest</th>\n",
              "      <th>swedenhyeju</th>\n",
              "      <th>greeceg</th>\n",
              "      <th>inform</th>\n",
              "      <th>compos</th>\n",
              "      <th>inf</th>\n",
              "      <th>patternstask</th>\n",
              "      <th>romaniabogd</th>\n",
              "      <th>stateoftheart</th>\n",
              "      <th>two thousand and seventeen</th>\n",
              "      <th>changesport</th>\n",
              "      <th>compil</th>\n",
              "      <th>stockholm</th>\n",
              "      <th>attend</th>\n",
              "      <th>mat</th>\n",
              "      <th>hammad</th>\n",
              "      <th>must</th>\n",
              "      <th>aud</th>\n",
              "      <th>two thousand and twenty</th>\n",
              "      <th>alberto</th>\n",
              "      <th>spainanastasio</th>\n",
              "      <th>francem</th>\n",
              "      <th>rapid</th>\n",
              "      <th>negoty</th>\n",
              "      <th>docu</th>\n",
              "      <th>comp</th>\n",
              "      <th>...</th>\n",
              "      <th>soph</th>\n",
              "      <th>romain</th>\n",
              "      <th>short</th>\n",
              "      <th>techn</th>\n",
              "      <th>ent</th>\n",
              "      <th>object</th>\n",
              "      <th>fut</th>\n",
              "      <th>fil</th>\n",
              "      <th>bioasq</th>\n",
              "      <th>larg</th>\n",
              "      <th>conceiv</th>\n",
              "      <th>changyu</th>\n",
              "      <th>portugalrafael</th>\n",
              "      <th>gwenaël</th>\n",
              "      <th>twentysev</th>\n",
              "      <th>pract</th>\n",
              "      <th>ked</th>\n",
              "      <th>auth</th>\n",
              "      <th>on</th>\n",
              "      <th>pubm</th>\n",
              "      <th>kim</th>\n",
              "      <th>trento</th>\n",
              "      <th>diff</th>\n",
              "      <th>constantin</th>\n",
              "      <th>grow</th>\n",
              "      <th>røst</th>\n",
              "      <th>parissaclay</th>\n",
              "      <th>in</th>\n",
              "      <th>eg</th>\n",
              "      <th>barro</th>\n",
              "      <th>org</th>\n",
              "      <th>issu</th>\n",
              "      <th>may</th>\n",
              "      <th>seventytwo</th>\n",
              "      <th>park</th>\n",
              "      <th>usayonghu</th>\n",
              "      <th>norweg</th>\n",
              "      <th>cho</th>\n",
              "      <th>backgroundth</th>\n",
              "      <th>thirtysix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00129</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00086</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.00043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.001984</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 727 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   supercomput      sous  memorabilitysc  ...      cho  backgroundth  thirtysix\n",
              "0     0.000000  0.000000         0.00043  ...  0.00043       0.00043    0.00043\n",
              "1     0.000992  0.000496         0.00000  ...  0.00000       0.00000    0.00000\n",
              "\n",
              "[2 rows x 727 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ5D53UA1oiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sort the scores (to know what words are more relevant in each document)\n",
        "\n",
        "#Document 1 (Web page 1)\n",
        "score1 = pd.DataFrame([tfidfBowA])\n",
        "scorest1 = score1.T\n",
        "scorest1.columns = ['tf.idf(doc1)']\n",
        "\n",
        "scorestf1 = scorest1.sort_values(['tf.idf(doc1)'], ascending = False)\n",
        "#print(scorestf1)\n",
        "\n",
        "#-------\n",
        "\n",
        "#Document 2 (Web page 2)\n",
        "score2 = pd.DataFrame([tfidfBowB])\n",
        "scorest2 = score2.T\n",
        "scorest2.columns = ['tf.idf(doc2)']\n",
        "\n",
        "scorestf2 = scorest2.sort_values(['tf.idf(doc2)'], ascending = False)\n",
        "#print(scorestf2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Ii5TTK1oiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Exportin the results to csv\n",
        "\n",
        "scorestf1.to_csv('/Users/davidalbuja/Documents/Essex/Spring/Information Retrieval/Assigment 1/Outputs/TFIDF(doc1).csv')\n",
        "scorestf2.to_csv('/Users/davidalbuja/Documents/Essex/Spring/Information Retrieval/Assigment 1/Outputs/TFIDF(doc2).csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBSGKbbN1oiP",
        "colab_type": "text"
      },
      "source": [
        "### Entity recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOnnd-O41oiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entity recognition\n",
        "from nltk import ne_chunk\n",
        "\n",
        "chu1 = ne_chunk(tag1, binary=True)\n",
        "chu2 = ne_chunk(tag2, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IuB71rR1oiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}